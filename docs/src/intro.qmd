
# Joint Energy Models

```{julia}
#| echo: false
include("docs/setup_docs.jl")
eval(setup_docs)
```

## Data

```{julia}
nobs = 10000
n_digits = 28

function _resize(x, size=(n_digits, n_digits))
    if n_digits != 28
        img_source = MLDatasets.convert2image(MNIST, x)
        img_rs = imresize(img_source, size)
        x = permutedims(convert(Array{Float32}, Gray.(img_rs)))
    end
    return x
end

function pre_process(x; noise::Float32=0.03f0)
    œµ = Float32.(randn(size(x)) * noise)
    x = @.(2 * x - 1) .+ œµ
    return x
end

# Train Set:
Xtrain, ytrain = MNIST(split=:train)[:]
Xtrain = Xtrain[:,:,1:nobs]
Xtrain = mapslices(x -> _resize(x), Xtrain, dims=(1,2)) 
ytrain = ytrain[1:nobs] 

# Test Set:
Xtest, ytest = MNIST(split=:test)[:]
Xtest = Xtest[:,:,1:nobs]
Xtest = mapslices(x -> _resize(x), Xtest, dims=(1,2)) 
ytest = ytest[1:nobs] 

## One-hot-encode the labels
ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)

## Validation Set:
num_val = Int(round(nobs / 10))
Xtrain, Xval = (Xtrain[:,:,1:(end-num_val)], Xtrain[:,:,(end-num_val+1):end])
Xtrain = mapslices(x -> pre_process(x), Xtrain, dims=(1,2)) 
Xval = mapslices(x -> pre_process(x, noise=0.0f0), Xval, dims=(1,2)) 
ytrain, yval = (ytrain[:, 1:(end-num_val)], ytrain[:, (end-num_val+1):end])
```

## `JointEnergyModel`

## Evaluation and Training

```{julia}
function accuracy(model::JointEnergyModel, x, y; agg=mean)
    yÃÇ = jem(x)
    mean(onecold(yÃÇ) .== onecold(y))
end

function evaluation(model::JointEnergyModel, val_set::DataLoader)
    ‚Ñì = 0.0
    ‚Ñì_clf = 0.0
    ‚Ñì_gen = 0.0
    acc = 0.0
    num = 0
    for (x, y) in val_set
        ‚Ñì_clf += sum(class_loss(model, x, y))
        ‚Ñì_gen += sum(gen_loss(model, x))
        ‚Ñì += loss(model, x, y)
        acc += accuracy(model, x, y)
        num += size(x)[end]
    end
    return ‚Ñì / num, ‚Ñì_clf / num, ‚Ñì_gen / num, acc / length(val_set)
end

function samples_real(model::JointEnergyModel, dl::DataLoader, n::Int=16; img_size=n_digits * 5)
    x = reduce((x, y) -> cat(x,y[1],dims=ndims(x)), dl, init = [])
    num_x = Int(round(ceil(sqrt(n))))
    num_y = Int(round(floor(sqrt(n))))
    plot_data = [heatmap(rotl90(x[:,:,rand(1:size(x)[end])]), axis=nothing, cb=false) for i in 1:n]
    plot(plot_data..., layout=(num_x, num_y), size=(num_x*img_size, num_y*img_size), margin=(round(0.05*img_size), :px))
end

function samples_generated(model::JointEnergyModel, dl::DataLoader, n::Int=16; img_size=n_digits * 5)
    x = reduce((x, y) -> cat(x,y[1],dims=ndims(x)), dl, init = [])
    x = jem.sampler(jem.chain, jem.sampling_rule, size(x))
    num_x = Int(round(ceil(sqrt(n))))
    num_y = Int(round(floor(sqrt(n))))
    plot_data = [heatmap(rotl90(x[:,:,i]), axis=nothing, cb=false) for i in 1:n]
    plot(plot_data..., layout=(num_x, num_y), size=(num_x*img_size, num_y*img_size), margin=(round(0.05*img_size), :px))
end
```

```{julia}
function training(
    model::JointEnergyModel, train_set, opt_state; 
    num_epochs::Int=100, val_set::Union{Nothing, DataLoader}=nothing, max_patience::Int=20,
    verbosity::Int=num_epochs
)
    training_log = []
    for epoch in 1:num_epochs
        training_losses = Float32[]

        # Training:
        for (i, data) in enumerate(train_set)

            # Forward pass:
            x, y = data
            val, grads = Flux.withgradient(model) do m
                loss(m, x, y)
            end

            # Save the loss from the forward pass. (Done outside of gradient.)
            push!(training_losses, val)

            # Detect loss of Inf or NaN. Print a warning, and then skip update!
            if !isfinite(val)
                @warn "loss is $val on item $i" epoch
                continue
            end

            Flux.update!(opt_state, model, grads[1])
        end

        # Evluation:
        if !isnothing(val_set)
            ‚Ñì, ‚Ñì_clf, ‚Ñì_gen, acc = evaluation(model, val_set)
            push!(training_log, (; ‚Ñì, ‚Ñì_clf, ‚Ñì_gen, acc, training_losses))
        else
            ‚Ñì, _, _, acc = evaluation(model, train_set)
            push!(training_log, (; ‚Ñì, acc, training_losses))
        end

        # Verbosity:
        if (verbosity > 0) && (epoch % round(num_epochs / verbosity) == 0)
            if isnothing(val_set)
                @info "Training loss in epoch $epoch: $‚Ñì"
                @info "Training accuracy in epoch $epoch: $acc"
            else
                @info "Validation losses/accuracy in epoch $epoch:"
                println("Classification: $‚Ñì_clf")
                println("Generative: $‚Ñì_gen")
                println("Total: $‚Ñì")
                println("Accuracy: $acc")
            end
        end

        # Early Stopping:
        _loss() = ‚Ñì
        es = Flux.early_stopping(_loss, max_patience)
        es() && break

    end

    return training_log
end
```

## Experiments

### Hyperparameters

```{julia}
D = n_digits               
K = 10                      
M = 32         
lr = 1e-4                  
num_epochs = 10
max_patience = 5            
batchsize = Int(round(nobs/10))
```

### Initializing the model

```{julia}
mlp = Chain(
    Flux.flatten,
    Dense(prod((D,D)), M, elu),
    # Dense(M, M, elu),
    # Dense(M, M, elu),
    Dense(M, K)
)

# We initialize the full model
ùíüx = Uniform(-1,1)
ùíüy = Categorical(ones(K) ./ K)
sampler = UnconditionalSampler(ùíüx)
jem = JointEnergyModel(mlp, sampler)
```

### Training loop

```{julia}
# Initialise 
opt = Adam(lr)
opt_state = Flux.setup(opt, jem)
train_set = DataLoader((Xtrain, ytrain); batchsize=batchsize, shuffle=true)
val_set = DataLoader((Xval, yval); batchsize=batchsize, shuffle=false)
test_set = DataLoader((Xtest, ytest); batchsize=batchsize, shuffle=false)
```

```{julia}
logs = training(jem, train_set, opt_state; num_epochs=num_epochs, val_set=val_set)
```

### The final evaluation

```{julia}
‚Ñì_test, ‚Ñì_clf_test, ‚Ñì_gen_test, acc_test = evaluation(jem, test_set)
```

