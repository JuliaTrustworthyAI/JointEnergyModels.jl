
# Joint Energy Models

Adapted from https://github.com/jmtomczak/intro_dgm/blob/main/ebms/ebm_example.ipynb

```{julia}
Pkg.activate("docs")
using Base: @kwdef
using Flux
using Flux.Data: DataLoader
using Flux: onehotbatch, onecold, @epochs, @functor, logsumexp
using Flux.Losses: logitcrossentropy
using MLDatasets
using Statistics
```

## Data

```{julia}
nobs = 1000

# Train Set:
Xtrain, ytrain = MNIST(split=:train)[:]
Xtrain = Xtrain[:,:,1:nobs]
ytrain = ytrain[1:nobs] 

# Test Set:
Xtest, ytest = MNIST(split=:test)[:]

## One-hot-encode the labels
ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)

## Validation Set:
num_val = Int(round(nobs / 10))
Xtrain, Xval = (Xtrain[:,:,1:(end-num_val)], Xtrain[:,:,(end-num_val+1):end])
ytrain, yval = (ytrain[:, 1:(end-num_val)], ytrain[:, (end-num_val+1):end])
```

## `JointEnergyModel`

```{julia}
struct JointEnergyModel
    chain::Chain
    α::Float32
    σ::Float32
    ld_steps::Int
end

function JointEnergyModel(chain::Chain; α::Real=1.0, σ::Real=0.01, ld_steps::Int=20)
    return JointEnergyModel(chain, α, σ, ld_steps)
end

Flux.@functor JointEnergyModel

function (jem::JointEnergyModel)(x)
    jem.chain(x)
end
```

```{julia}
function classify(jem::JointEnergyModel, x)
    ŷ = jem(x)
    p̂ = softmax(ŷ)
    return p̂
end

function class_loss(jem::JointEnergyModel, x, y)
    ŷ = jem(x)
    ℓ = logitcrossentropy(ŷ, y; agg=x -> x)
    return ℓ
end

function gen_loss(jem::JointEnergyModel, x)
    ŷ = jem(x)
    xsample = sample(jem, x)
    ŷsample = jem(xsample)
    ℓ = - (logsumexp(ŷ; dims=1) - logsumexp(ŷsample; dims=1))
    return ℓ
end

function loss(jem::JointEnergyModel, x, y; agg=mean)
    ℓ_clf = class_loss(jem, x, y)
    ℓ_gen = gen_loss(jem, x)
    loss = agg(ℓ_clf .+ ℓ_gen)
    return loss
end
```


```{julia}
function energy_gradient(jem::JointEnergyModel, x)
    xᵢ = deepcopy(x)
    ŷ = jem(xᵢ)
    f(x) = sum(logsumexp(jem(x)))
    gs = gradient(f, xᵢ)
    return gs
end

function langevine_dynamics_step(jem::JointEnergyModel, xold)
    # Calculate gradient wrt xold
    grad_energy = energy_gradient(jem, xold)[1]
    # Sample eta ~ Normal(0, alpha)
    ε = Float32.(randn(size(grad_energy)...) * jem.σ)
    # New sample
    xnew = @.(xold + jem.α * grad_energy + ε)

    return xnew
end

function sample(jem::JointEnergyModel, x)
    xsample = Float32.(rand(size(x)...))
    for i in 1:jem.ld_steps
        xsample = langevine_dynamics_step(jem, xsample)
    end
    return xsample
end
```

## Evaluation and Training

```{julia}
function evaluation(model::JointEnergyModel, val_set::DataLoader)
    ℓ = 0.0
    ℓ_clf = 0.0
    ℓ_gen = 0.0
    num = 0
    for (x, y) in val_set
        ℓ_clf += sum(class_loss(jem, x, y))
        ℓ_gen += sum(gen_loss(jem, x))
        ℓ += loss(jem, x, y)
        num += size(x)[end]
    end
    return ℓ / num, ℓ_clf / num, ℓ_gen / num
end
```

```{julia}
function training(
    model::JointEnergyModel, train_set, opt_state; 
    num_epochs::Int=100, val_set::Union{Nothing, DataLoader}=nothing, max_patience::Int=20,
    verbosity::Int=num_epochs
)
    training_log = []
    for epoch in 1:num_epochs
        losses = Float32[]

        # Training:
        for (i, data) in enumerate(train_set)

            # Forward pass:
            x, y = data
            val, grads = Flux.withgradient(model) do m
                loss(m, x, y)
            end

            # Save the loss from the forward pass. (Done outside of gradient.)
            push!(losses, val)

            # Detect loss of Inf or NaN. Print a warning, and then skip update!
            if !isfinite(val)
                @warn "loss is $val on item $i" epoch
                continue
            end

            Flux.update!(opt_state, model, grads[1])
        end

        # Evluation:
        if !isnothing(val_set)
            ℓ, ℓ_clf, ℓ_gen = evaluation(model, val_set)
            push!(training_log, (; losses, ℓ, ℓ_clf, ℓ_gen))
        else
            ℓ, _, _ = evaluation(model, train_set)
        end

        # Verbosity:
        if (verbosity > 0) && (epoch % round(num_epochs / verbosity) == 0)
            loss_type = isnothing(val_set) ? "Training" : "Validation"
            @info "$loss_type loss in epoch $epoch: $ℓ"
        end

        # Early Stopping:
        # Flux.early_stopping(ℓ, max_patience) && break

    end
end
```

## Experiments

### Hyperparameters

```{julia}
D = 28                      # input dimension
K = 10                      # output dimension
M = 512                     # the number of neurons
lr = 1e-3                   # learning rate
num_epochs = 1000           # max. number of epochs
max_patience = 20           # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped
batchsize = 64              # mini-batch size
```

### Initializing the model

```{julia}
mlp = Chain(
    Flux.flatten,
    Dense(prod((D,D)), M, relu),
    Dense(M, K)
)

# We initialize the full model
jem = JointEnergyModel(mlp)
```

### Training loop

```{julia}
# Initialise 
opt = AdaMax(lr)
opt_state = Flux.setup(opt, jem)
train_set = DataLoader((Xtrain, ytrain); batchsize=batchsize, shuffle=true)
val_set = DataLoader((Xval, yval); batchsize=batchsize, shuffle=false)
test_set = DataLoader((Xtest, ytest); batchsize=batchsize, shuffle=false)
```

```{julia}
training(jem, train_set, opt_state; num_epochs=num_epochs, val_set=val_set)
```


