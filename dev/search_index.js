var documenterSearchIndex = {"docs":
[{"location":"tutorials/mnist/#Joint-Energy-Models","page":"Joint Energy Models","title":"Joint Energy Models","text":"","category":"section"},{"location":"tutorials/mnist/#Data","page":"Joint Energy Models","title":"Data","text":"nobs = 1000\nn_digits = 28\nXtrain, ytrain, Xval, yval, Xtest, ytest = load_mnist_data(nobs=nobs, n_digits=n_digits)","category":"section"},{"location":"tutorials/mnist/#JointEnergyModel","page":"Joint Energy Models","title":"JointEnergyModel","text":"","category":"section"},{"location":"tutorials/mnist/#Hyperparameters","page":"Joint Energy Models","title":"Hyperparameters","text":"D = n_digits               \nK = 10                      \nM = 128\nlr = 1e-3                \nnum_epochs = 100\nmax_patience = 5            \nbatchsize = Int(round(nobs/10))","category":"section"},{"location":"tutorials/mnist/#Initializing-the-model","page":"Joint Energy Models","title":"Initializing the model","text":"activation = relu\nmlp = Chain(\n    MLUtils.flatten,\n    Dense(prod((D,D)), M, activation),\n    # BatchNorm(M, activation),\n    # Dense(M, M),\n    # BatchNorm(M, activation),\n    Dense(M, K),\n)\n\n# We initialize the full model\nùíüx = Uniform(-1,1)\nùíüy = Categorical(ones(K) ./ K)\nsampler = ConditionalSampler(ùíüx, ùíüy, input_size=(D,D), batch_size=10)\njem = JointEnergyModel(\n    mlp, sampler;\n    sampling_steps=20,\n)","category":"section"},{"location":"tutorials/mnist/#Training-loop","page":"Joint Energy Models","title":"Training loop","text":"# Initialise \nopt = Adam(lr)\nopt_state = Flux.setup(opt, jem)\ntrain_set = DataLoader((Xtrain, ytrain); batchsize=batchsize, shuffle=true)\nval_set = DataLoader((Xval, yval); batchsize=batchsize, shuffle=false)\ntest_set = DataLoader((Xtest, ytest); batchsize=batchsize, shuffle=false)\n\nlogs = train_model(\n    jem, train_set, opt_state; num_epochs=num_epochs, val_set=val_set,\n    verbosity = minimum([num_epochs, 10]),\n    Œ± = [1.0,1.0,1e-2],\n    # use_class_loss=false,\n    # use_gen_loss=false,\n    # use_reg_loss=false,\n)","category":"section"},{"location":"tutorials/mnist/#The-final-evaluation","page":"Joint Energy Models","title":"The final evaluation","text":"n_iter = 200\n_w = 1500\nplts = []\nneach = 10\nfor i in 1:10\n    x = jem.sampler(jem.chain, jem.sampling_rule; niter=n_iter, n_samples=neach, y=i)\n    plts_i = []\n    for j in 1:size(x, 3)\n        xj = x[:,:,j]\n        plts_i = [plts_i..., heatmap(rotl90(xj), axis=nothing, cb=false)]\n    end\n    plt = plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))\n    plts = [plts..., plt]\nend\nplot(plts..., size=(_w,_w), layout=(10,1))","category":"section"},{"location":"tutorials/mnist/#From-Scratch","page":"Joint Energy Models","title":"From Scratch","text":"sampler = UnconditionalSampler(ùíüx; input_size=(D,D))\nconditional_sampler = ConditionalSampler(ùíüx, ùíüy; input_size=(D,D))\nopt = ImproperSGLD(10.0,0.005)\nn_iter = 256\n\n_w = 1500\nplts = []\nneach = 10\nfor i in 1:10\n    x = conditional_sampler(jem.chain, opt; niter=n_iter, y=i, n_samples=neach)\n    plts_i = []\n    for j in 1:size(x,3)\n        xj = x[:,:,j]\n        plts_i = [plts_i..., heatmap(rotl90(xj), axis=nothing, cb=false)]\n    end\n    plt = plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))\n    plts = [plts..., plt]\nend\nplot(plts..., size=(_w,_w), layout=(10,1))\n\n_w = 1500\nplts = []\nneach = 10\nfor i in 1:10\n    x = sampler(jem.chain, opt; niter=n_iter, y=i, n_samples=neach)\n    plts_i = []\n    for j in 1:size(x,3)\n        xj = x[:,:,j]\n        plts_i = [plts_i..., heatmap(rotl90(xj), axis=nothing, cb=false)]\n    end\n    plt = plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))\n    plts = [plts..., plt]\nend\nplot(plts..., size=(_w,_w), layout=(10,1))","category":"section"},{"location":"explanation/samplers/#MNIST","page":"MNIST","title":"MNIST","text":"using TaijaData: load_mnist\nusing CounterfactualExplanations.Models: load_mnist_mlp, load_mnist_ensemble\n\nX, y = load_mnist()\nK = size(y, 1)\nD = size(X, 1)\nmlp = load_mnist_mlp().model\nf_mlp(x) = mlp(x)\nens = load_mnist_ensemble().model\nf_ens(x) = sum(map(mlp -> mlp(x), ens))/length(ens)\nbatch_size = 100","category":"section"},{"location":"explanation/samplers/#Sampling","page":"MNIST","title":"Sampling","text":"ùíüx = Uniform(0,1)\nùíüy = Categorical(ones(K) ./ K)\nsampler = UnconditionalSampler(ùíüx; input_size=(D,))\nconditional_sampler = ConditionalSampler(ùíüx, ùíüy; input_size=(D,))\nopt = ImproperSGLD()\nn_iter = 256","category":"section"},{"location":"explanation/samplers/#Conditional-Draws","page":"MNIST","title":"Conditional Draws","text":"_w = 1500\nplts = []\nneach = 10\nfor i in 1:10\n    x = conditional_sampler(f_mlp, opt; niter=n_iter, y=i, n_samples=neach)\n    plts_i = []\n    for j in 1:size(x,2)\n        xj = reshape(x[:,j], (28,28))\n        plts_i = [plts_i..., heatmap(rotl90(xj), axis=nothing, cb=false)]\n    end\n    plt = plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))\n    plts = [plts..., plt]\nend\nplot(plts..., size=(_w,_w), layout=(10,1))\n\n_w = 1500\nplts = []\nneach = 10\nfor i in 1:10\n    x = conditional_sampler(f_ens, opt; niter=n_iter, y=i, n_samples=neach)\n    plts_i = []\n    for j in 1:size(x,2)\n        xj = reshape(x[:,j], (28,28))\n        plts_i = [plts_i..., heatmap(rotl90(xj), axis=nothing, cb=false)]\n    end\n    plt = plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))\n    plts = [plts..., plt]\nend\nplot(plts..., size=(_w,_w), layout=(10,1))","category":"section"},{"location":"explanation/samplers/#Unconditional-Draws","page":"MNIST","title":"Unconditional Draws","text":"_w = 1500\nplts = []\nneach = 10\nfor i in 1:10\n    x = sampler(f_mlp, opt; niter=n_iter, n_samples=neach)\n    plts_i = []\n    for j in 1:size(x,2)\n        xj = reshape(x[:,j], (28,28))\n        plts_i = [plts_i..., heatmap(rotl90(xj), axis=nothing, cb=false)]\n    end\n    plt = plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))\n    plts = [plts..., plt]\nend\nplot(plts..., size=(_w,_w), layout=(10,1))\n\n_w = 1500\nplts = []\nneach = 10\nfor i in 1:10\n    x = sampler(f_ens, opt; niter=n_iter, n_samples=neach)\n    plts_i = []\n    for j in 1:size(x,2)\n        xj = reshape(x[:,j], (28,28))\n        plts_i = [plts_i..., heatmap(rotl90(xj), axis=nothing, cb=false)]\n    end\n    plt = plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))\n    plts = [plts..., plt]\nend\nplot(plts..., size=(_w,_w), layout=(10,1))","category":"section"},{"location":"assets/resources/#Further-Resources","page":"Further Resources","title":"Further Resources","text":"","category":"section"},{"location":"assets/resources/#JuliaCon-2022","page":"Further Resources","title":"JuliaCon 2022","text":"Slides: link","category":"section"},{"location":"assets/resources/#JuliaCon-Proceedings-Paper","page":"Further Resources","title":"JuliaCon Proceedings Paper","text":"TBD","category":"section"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"In this reference, you will find a detailed overview of the package API.\n\nReference guides are technical descriptions of the machinery and how to operate it. Reference material is information-oriented.‚Äî Di√°taxis\n\nIn other words, you come here because you want to take a very close look at the code üßê.","category":"section"},{"location":"reference/#Content","page":"Reference","title":"Content","text":"Pages = [\"reference.md\"]\nDepth = 3","category":"section"},{"location":"reference/#Exported-functions","page":"Reference","title":"Exported functions","text":"","category":"section"},{"location":"reference/#Internal-functions","page":"Reference","title":"Internal functions","text":"","category":"section"},{"location":"reference/#EnergySamplers.ConditionalSampler-Tuple{Union{Tables.MatrixTable, AbstractMatrix}, Union{CategoricalArrays.CategoricalArray, AbstractMatrix}}","page":"Reference","title":"EnergySamplers.ConditionalSampler","text":"ConditionalSampler(\n    X::AbstractArray, y::AbstractArray;\n    batch_size::Int,\n    max_len::Int=10000, prob_buffer::AbstractFloat=0.95\n)\n\nOuter constructor for ConditionalSampler.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JointEnergyModels.JointEnergyClassifier","page":"Reference","title":"JointEnergyModels.JointEnergyClassifier","text":"The JointEnergyClassifier struct is a wrapper for a JointEnergyModel that can be used with MLJFlux.jl.\n\n\n\n\n\n","category":"type"},{"location":"reference/#JointEnergyModels.class_loss-Tuple{JointEnergyModel, Any, Any}","page":"Reference","title":"JointEnergyModels.class_loss","text":"class_loss(jem::JointEnergyModel, x, y)\n\nComputes the classification loss.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JointEnergyModels.gen_loss-Tuple{JointEnergyModel, Any, Any}","page":"Reference","title":"JointEnergyModels.gen_loss","text":"gen_loss(jem::JointEnergyModel, x)\n\nComputes the generative loss.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JointEnergyModels.generate_conditional_samples-Tuple{JointEnergyModel, Int64, Int64}","page":"Reference","title":"JointEnergyModels.generate_conditional_samples","text":"generate_conditional_samples(model, rule::Flux.Optimise.AbstractOptimiser, n::Int, y::Int; kwargs...)\n\nA convenience function for generating conditional samples for a given model, sampler and sampling rule. If n is missing, then the sampler's batch_size is used. The conditioning value y needs to be specified.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JointEnergyModels.generate_samples-Tuple{JointEnergyModel, Int64}","page":"Reference","title":"JointEnergyModels.generate_samples","text":"generate_samples(jem::JointEnergyModel, n::Int; kwargs...)\n\nA convenience function for generating samples for a given energy model. If n is missing, then the sampler's batch_size is used.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JointEnergyModels.loss-Tuple{JointEnergyModel, Any, Any}","page":"Reference","title":"JointEnergyModels.loss","text":"loss(jem::JointEnergyModel, x, y; agg=mean)\n\nComputes the total loss.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JointEnergyModels.get_samples-Tuple{JointEnergyModel, Any}","page":"Reference","title":"JointEnergyModels.get_samples","text":"get_samples(jem::JointEnergyModel, x)\n\nGets samples from the sampler buffer.\n\n\n\n\n\n","category":"method"},{"location":"reference/#JointEnergyModels.reg_loss-Tuple{JointEnergyModel, Any, Any}","page":"Reference","title":"JointEnergyModels.reg_loss","text":"reg_loss(jem::JointEnergyModel, x)\n\nComputes the regularization loss.\n\n\n\n\n\n","category":"method"},{"location":"assets/resources-commonmark/#Further-Resources","page":"Further Resources","title":"Further Resources","text":"","category":"section"},{"location":"assets/resources-commonmark/#JuliaCon-2022","page":"Further Resources","title":"JuliaCon 2022","text":"Slides: link","category":"section"},{"location":"assets/resources-commonmark/#JuliaCon-Proceedings-Paper","page":"Further Resources","title":"JuliaCon Proceedings Paper","text":"TBD","category":"section"},{"location":"how_to_guides/mlj_flux/#Compatibility-with-MLJFlux","page":"Compatibility with MLJFlux","title":"Compatibility with MLJFlux","text":"","category":"section"},{"location":"how_to_guides/mlj_flux/#Synthetic-Data","page":"Compatibility with MLJFlux","title":"Synthetic Data","text":"nobs=2000\nX, y = make_circles(nobs, noise=0.1, factor=0.5)\nXplot = Float32.(permutedims(matrix(X)))\nX = table(permutedims(Xplot))\ndisplay(scatter(Xplot[1,:], Xplot[2,:], group=y, label=\"\"))\nbatch_size = Int(round(nobs/10))\n\nsampler = ConditionalSampler(X, y, batch_size=batch_size)\nclf = JointEnergyClassifier(\n    sampler;\n    builder=MLJFlux.MLP(hidden=(32, 32, 32,), œÉ=Flux.relu),\n    batch_size=batch_size,\n    finaliser=Flux.softmax,\n    loss=Flux.Losses.crossentropy,\n    jem_training_params=(Œ±=[1.0,1.0,1e-1],verbosity=10,),\n)\n\nprintln(typeof(clf) <: MLJFlux.MLJFluxModel)\n\nmach = machine(clf, X, y)\nfit!(mach)\n\njem = mach.model.jem\nbatch_size = mach.model.batch_size\nX = Float32.(permutedims(matrix(X)))\ny_labels = Int.(y.refs)\ny = Flux.onehotbatch(y.refs, sort(unique(y_labels)))\n\nif typeof(jem.sampler) <: ConditionalSampler\n    \n    plts = []\n    for target in 1:size(y,1)\n        XÃÇ = generate_conditional_samples(jem, batch_size, target; niter=1000) \n        ex = extrema(hcat(X,XÃÇ), dims=2)\n        xlims = ex[1]\n        ylims = ex[2]\n        x1 = range(1.0f0.*xlims...,length=100)\n        x2 = range(1.0f0.*ylims...,length=100)\n        plt = contour(\n            x1, x2, (x, y) -> softmax(jem([x, y]))[target], \n            fill=true, alpha=0.5, title=\"Target: $target\", cbar=false,\n            xlims=xlims,\n            ylims=ylims,\n        )\n        scatter!(X[1,:], X[2,:], color=vec(y_labels), group=vec(y_labels), alpha=0.5)\n        scatter!(\n            XÃÇ[1,:], XÃÇ[2,:], \n            color=repeat([target], size(XÃÇ,2)), \n            group=repeat([target], size(XÃÇ,2)), \n            shape=:star5, ms=10\n        )\n        push!(plts, plt)\n    end\n    plt = plot(plts..., layout=(1, size(y,1)), size=(size(y,1)*400, 400))\n    display(plt)\nend","category":"section"},{"location":"how_to_guides/mlj_flux/#MNIST","page":"Compatibility with MLJFlux","title":"MNIST","text":"# Data:\nnobs = 1000\nn_digits = 28\nXtrain, ytrain, _, _, _, _ = load_mnist_data(nobs=nobs, n_digits=n_digits)\nXtrain = table(permutedims(MLUtils.flatten(Xtrain)))\nytrain = coerce(Flux.onecold(ytrain, 0:9), Multiclass)\n\n# Hyperparameters:\nD = n_digits^2             \nK = 10                      \nM = 32\nlr = 1e-3           \nnum_epochs = 500\nmax_patience = 5            \nbatch_size = Int(round(nobs/10))\nŒ± = [1.0,1.0,1e-2]\n\nactivation = Flux.swish\nbuilder = MLJFlux.MLP(hidden=(M,M,M,), œÉ=activation)\n\n# We initialize the full model\nùíüx = Uniform(-1,1)\nùíüy = Categorical(ones(K) ./ K)\nsampler = ConditionalSampler(ùíüx, ùíüy, input_size=(D,), batch_size=10)\nclf = JointEnergyClassifier(\n    sampler;\n    builder=builder,\n    batch_size=batch_size,\n    finaliser=Flux.softmax,\n    loss=Flux.Losses.crossentropy,\n    jem_training_params=(Œ±=Œ±,verbosity=10,),\n    sampling_steps=20,\n    optimiser=Flux.Optimise.Adam(lr),\n)\n\nmach = machine(clf, Xtrain, ytrain)\nfit!(mach)\n\njem = mach.model.jem\nn_iter = 1000\n_w = 1500\nplts = []\nneach = 10\nfor i in 1:10\n    x = jem.sampler(jem.chain, jem.sampling_rule; niter=n_iter, n_samples=neach, y=i)\n    plts_i = []\n    for j in 1:size(x, 2)\n        xj = x[:,j]\n        xj = reshape(xj, (n_digits, n_digits))\n        plts_i = [plts_i..., heatmap(rotl90(xj), axis=nothing, cb=false)]\n    end\n    plt = plot(plts_i..., size=(_w,0.10*_w), layout=(1,10))\n    plts = [plts..., plt]\nend\nplot(plts..., size=(_w,_w), layout=(10,1))","category":"section"},{"location":"#JointEnergyModels.jl","page":"üè† Home","title":"JointEnergyModels.jl","text":"Documentation for JointEnergyModels.jl.\n\nJoint Energy Models in Julia.\n\n(Image: Stable) (Image: Dev) (Image: Build Status) (Image: Coverage) (Image: Code Style: Blue) (Image: License) (Image: Package Downloads) (Image: Aqua QA)\n\nJointEnergyModels.jl is a package for training Joint Energy Models in Julia. Joint Energy Models (JEM) are hybrid models that learn to discriminate between classes y and generate input data x. They were introduced in Grathwohl et al. (2020), which provides the foundation for the methodologies implemented in this package.","category":"section"},{"location":"#Status","page":"üè† Home","title":"üîÅ Status","text":"This package is still in its infancy and the API is subject to change. Currently, the package can be used to train JEMs for classification. It is also possible to train pure Energy-Based Models (EBMs) for the generative task only. The package is compatible with Flux.jl. Work on compatibility with MLJ.jl (through MLJFlux.jl) is currently under way.\n\nWe welcome contributions and feedback at this early stage. To install the development version of the package you can run the following command:\n\nusing Pkg\nPkg.add(url=\"https://github.com/juliatrustworthyai/JointEnergyModels.jl\")","category":"section"},{"location":"#Usage-Example","page":"üè† Home","title":"üîç Usage Example","text":"Below we first generate some synthetic data:\n\nnobs=2000\nX, y = make_circles(nobs, noise=0.1, factor=0.5)\nXplot = Float32.(permutedims(matrix(X)))\nX = table(permutedims(Xplot))\nplt = scatter(Xplot[1,:], Xplot[2,:], group=y, label=\"\")\nbatch_size = Int(round(nobs/10))\ndisplay(plt)\n\n(Image: )\n\nThe MLJ compatible classifier can be instantiated as follows:\n\nùíüx = Normal()\nùíüy = Categorical(ones(2) ./ 2)\nsampler = ConditionalSampler(ùíüx, ùíüy, input_size=size(Xplot)[1:end-1], batch_size=batch_size)\nclf = JointEnergyClassifier(\n    sampler;\n    builder=MLJFlux.MLP(hidden=(32, 32, 32,), œÉ=Flux.relu),\n    batch_size=batch_size,\n    finaliser=x -> x,\n    loss=Flux.Losses.logitcrossentropy,\n)\n\nIt uses the MLJFlux package to build the model:\n\nprintln(typeof(clf) <: MLJFlux.MLJFluxModel)\n\ntrue\n\nThe model can be wrapped in data and trained using the fit! function:\n\nmach = machine(clf, X, y)\nfit!(mach)\n\nThe results are visualised below. The model has learned to discriminate between the two classes (as indicated by the contours) and to generate samples from each class (as indicated by the stars).\n\n(Image: )","category":"section"},{"location":"#References","page":"üè† Home","title":"üéì References","text":"Grathwohl, Will, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. 2020. ‚ÄúYour Classifier Is Secretly an Energy Based Model and You Should Treat It Like One.‚Äù In. https://openreview.net/forum?id=Hkxzx0NtDB.","category":"section"},{"location":"tutorials/synthetic/#Synthetic-Data","page":"Synthetic Data","title":"Synthetic Data","text":"","category":"section"},{"location":"tutorials/synthetic/#Binary-Classification","page":"Synthetic Data","title":"Binary Classification","text":"nobs=2000\nX, y = make_circles(nobs, noise=0.1, factor=0.5)\nX = Float32.(permutedims(matrix(X)))\ny_labels = Int.(y.refs)\ny = Flux.onehotbatch(y.refs, sort(unique(y_labels)))\ndisplay(scatter(X[1,:], X[2,:], color=vec(y_labels), label=\"\"))\nbatch_size = Int(round(nobs/10))\ntrain_set = DataLoader((X, y), batchsize=batch_size, shuffle=true)\n\nn_hidden = 32\nactivation = relu\nmodels = Dict(\n    \"Logistic Regression\" => Chain(Dense(2, size(y,1))),\n    \"MLP\" => Chain(\n        Dense(2, n_hidden, activation), \n        Dense(n_hidden, n_hidden, activation), \n        Dense(n_hidden, n_hidden, activation), \n        Dense(n_hidden, size(y,1))\n    ),\n)\n\n_loss(y_hat, y) = Flux.Losses.logitcrossentropy(y_hat, y)\nrule = Adam()\n\n_lims = extrema(X, dims=2)\nx1, x2 = map(ex -> range(1.1f0.*ex..., length=100), _lims)\nn_epochs = 100\nplts = []\nfor (name, model) in models\n    opt_state = Flux.setup(rule, model)\n    for epoch in 1:n_epochs\n        Flux.train!(model, train_set, opt_state) do m, x, y\n            _loss(model(x), y)\n        end\n    end\n    plt = contour(x1, x2, (x, y) -> softmax(model([x, y]))[1], fill=true, alpha=0.5, title=name, cbar=false)\n    scatter!(X[1,:], X[2,:], color=vec(y_labels), label=\"\")\n    push!(plts, plt)\nend\nplot(plts..., layout=(1, 2), size=(800, 400))","category":"section"},{"location":"tutorials/synthetic/#Joint-Energy-Model","page":"Synthetic Data","title":"Joint Energy Model","text":"# We initialize the full model:\nùíüx = Normal()\nùíüy = Categorical(ones(2) ./ 2)\nsampler = ConditionalSampler(ùíüx, ùíüy, input_size=size(X)[1:end-1], batch_size=batch_size)\njem = JointEnergyModel(\n    models[\"MLP\"], \n    sampler;\n    sampling_rule=ImproperSGLD(),\n    sampling_steps=10\n)\n# Initialise training:\nopt = Adam(1e-3)\nopt_state = Flux.setup(opt, jem)\nnum_epochs = 100\n\nlogs = JointEnergyModels.train_model(\n    jem, train_set, opt_state; \n    num_epochs=num_epochs,\n    Œ±=[1.0,1.0,1e-1],\n    verbosity=minimum([num_epochs, 50]),\n    # use_class_loss=false,\n    # use_gen_loss=false,\n    # use_reg_loss=false,\n)\n\nplts = []\nfor target in 1:size(y,1)\n    plt = contour(x1, x2, (x, y) -> softmax(jem([x, y]))[target], fill=true, alpha=0.5, title=\"Target: $target\", cbar=false)\n    scatter!(X[1,:], X[2,:], color=vec(y_labels), group=vec(y_labels))\n    push!(plts, plt)\nend\nplot(plts..., layout=(1, size(y,1)), size=(size(y,1)*400, 400))\n\nXÃÇ = generate_samples(jem, 1000; niter=1000)\nyÃÇ = onecold(softmax(jem(XÃÇ)))\nscatter(X[1,:], X[2,:], color=vec(y_labels), group=vec(y_labels), alpha=0.5)\nscatter!(XÃÇ[1,:], XÃÇ[2,:], color=vec(yÃÇ), group=vec(yÃÇ), title=\"Generated Samples\", shape=:star5)\n\nif typeof(jem.sampler) <: ConditionalSampler\n    \n    plts = []\n    for target in 1:size(y,1)\n        XÃÇ = generate_conditional_samples(jem, batch_size, target; niter=1000) \n        ex = extrema(hcat(X,XÃÇ), dims=2)\n        xlims = ex[1]\n        ylims = ex[2]\n        x1 = range(1.0f0.*xlims...,length=100)\n        x2 = range(1.0f0.*ylims...,length=100)\n        plt = contour(\n            x1, x2, (x, y) -> softmax(jem([x, y]))[target], \n            fill=true, alpha=0.5, title=\"Target: $target\", cbar=false,\n            xlims=xlims,\n            ylims=ylims,\n        )\n        scatter!(X[1,:], X[2,:], color=vec(y_labels), group=vec(y_labels), alpha=0.5)\n        scatter!(\n            XÃÇ[1,:], XÃÇ[2,:], \n            color=repeat([target], size(XÃÇ,2)), \n            group=repeat([target], size(XÃÇ,2)), \n            shape=:star5, ms=10\n        )\n        push!(plts, plt)\n    end\n    plot(plts..., layout=(1, size(y,1)), size=(size(y,1)*400, 400))\nend","category":"section"}]
}
