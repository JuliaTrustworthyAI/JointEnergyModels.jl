# Synthetic Data

```{julia}
#| echo: false
include("docs/setup_docs.jl")
eval(setup_docs)
```

## Binary Classification

```{julia}
X, y = make_blobs(1000, 2, centers=3)
X = Float32.(permutedims(matrix(X)))
y_labels = Int.(y.refs)
y = Flux.onehotbatch(y, sort(unique(y_labels)))
display(scatter(X[1,:], X[2,:], color=vec(y_labels), label=""))
batch_size = 32
train_set = DataLoader((X, y), batchsize=batch_size, shuffle=true)
```

```{julia}
models = Dict(
    "Logistic Regression" => Chain(Dense(2, 3)),
    "MLP" => Chain(Dense(2, 10, elu), Dense(10, 3)),
)
```

```{julia}
loss(y_hat, y) = Flux.Losses.logitcrossentropy(y_hat, y)
rule = Descent(0.1)
```

```{julia}
_lims = extrema(X, dims=2)
x1, x2 = map(ex -> range(1.1f0.*ex..., length=100), _lims)
n_epochs = 100
plts = []
for (name, model) in models
    opt_state = Flux.setup(rule, model)
    for epoch in 1:n_epochs
        Flux.train!(model, train_set, opt_state) do m, x, y
            loss(model(x), y)
        end
    end
    plt = contour(x1, x2, (x, y) -> softmax(model([x, y]))[1], fill=true, alpha=0.5, title=name, cbar=false)
    scatter!(X[1,:], X[2,:], color=vec(y_labels), label="")
    push!(plts, plt)
end
plot(plts..., layout=(1, 2), size=(800, 400))
```

## Joint Energy Model

```{julia}
# We initialize the full model:
𝒟x = Normal()
𝒟y = Categorical(ones(2) ./ 2)
sampler = ConditionalSampler(𝒟x, 𝒟y, input_size=size(X)[1:end-1], batch_size=batch_size)
jem = JointEnergyModel(models["MLP"], sampler)
# Initialise training:
opt = Descent()
opt_state = Flux.setup(opt, jem)
num_epochs = 100
```

```{julia}
logs = JointEnergyModels.train_model(
    jem, train_set, opt_state; num_epochs=num_epochs
)
```


```{julia}
plt = contour(x1, x2, (x, y) -> softmax(jem([x, y]))[2], fill=true, alpha=0.5, title=name, cbar=true)
scatter!(X[1,:], X[2,:], color=vec(y_labels), label="")
```