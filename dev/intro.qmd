
# Joint Energy Models

Adapted from https://github.com/jmtomczak/intro_dgm/blob/main/ebms/ebm_example.ipynb

```{julia}
Pkg.activate("docs")
using Base: @kwdef
using Flux
using Flux.Data: DataLoader
using Flux: onehotbatch, onecold, @epochs, @functor, logsumexp
using Flux.Losses: logitcrossentropy
using Images
using MLDatasets
using Plots
using Plots.PlotMeasures
using Statistics
```

## Data

```{julia}
nobs = 1500
n_digits = 8

function _resize(x, size=(n_digits, n_digits))
    img_source = MLDatasets.convert2image(MNIST, x)
    img_rs = imresize(img_source, size)
    x = permutedims(convert(Array{Float32}, Gray.(img_rs)))
    return x
end

function pre_process(x; noise::Float32=0.03f0)
    ϵ = Float32.(randn(size(x)) * noise)
    x = @.(2 * x - 1) .+ ϵ
    return x
end

# Train Set:
Xtrain, ytrain = MNIST(split=:train)[:]
Xtrain = Xtrain[:,:,1:nobs]
Xtrain = mapslices(x -> _resize(x), Xtrain, dims=(1,2)) 
ytrain = ytrain[1:nobs] 

# Test Set:
Xtest, ytest = MNIST(split=:test)[:]
Xtest = Xtest[:,:,1:nobs]
Xtest = mapslices(x -> _resize(x), Xtest, dims=(1,2)) 
ytest = ytest[1:nobs] 

## One-hot-encode the labels
ytrain, ytest = onehotbatch(ytrain, 0:9), onehotbatch(ytest, 0:9)

## Validation Set:
num_val = Int(round(nobs / 10))
Xtrain, Xval = (Xtrain[:,:,1:(end-num_val)], Xtrain[:,:,(end-num_val+1):end])
Xtrain = mapslices(x -> pre_process(x), Xtrain, dims=(1,2)) 
Xval = mapslices(x -> pre_process(x, noise=0.0f0), Xval, dims=(1,2)) 
ytrain, yval = (ytrain[:, 1:(end-num_val)], ytrain[:, (end-num_val+1):end])
```

## `JointEnergyModel`

```{julia}
struct JointEnergyModel
    chain::Chain
    α::Float32
    σ::Float32
    ld_steps::Int
end

function JointEnergyModel(chain::Chain; α::Float32=1.0f0, σ::Float32=0.01f0, ld_steps::Int=20)
    return JointEnergyModel(chain, α, σ, ld_steps)
end

Flux.@functor JointEnergyModel

function (jem::JointEnergyModel)(x)
    jem.chain(x)
end
```

```{julia}
function class_loss(jem::JointEnergyModel, x, y)
    ŷ = jem(x)
    ℓ = logitcrossentropy(ŷ, y; agg=x -> x)
    return ℓ
end

function gen_loss(jem::JointEnergyModel, x)
    ŷ = jem(x)
    xsample = sample(jem, x)
    ŷsample = jem(xsample)
    ℓ = -(logsumexp(ŷ; dims=1) .- logsumexp(ŷsample; dims=1))
    return ℓ
end

function loss(jem::JointEnergyModel, x, y; agg=mean)
    ℓ_clf = class_loss(jem, x, y)
    ℓ_gen = gen_loss(jem, x)
    loss = agg(ℓ_clf .+ ℓ_gen)
    return loss
end
```


```{julia}
function energy_gradient(jem::JointEnergyModel, x)
    xᵢ = deepcopy(x)
    # f(x) = sum(logsumexp(jem(x); dims=1))
    f(x) = sum(map(y -> logsumexp(y), eachcol(jem(x))))
    gs = gradient(f, xᵢ)[1]
    return gs
end

function langevine_dynamics_step(jem::JointEnergyModel, x)
    # Calculate gradient wrt xold
    grad_energy = energy_gradient(jem, x)
    # Sample eta ~ Normal(0, alpha)
    ε = Float32.(randn(size(grad_energy)...) .* jem.σ)
    # New sample
    x += jem.α * grad_energy .+ ε

    return x
end

function sample(jem::JointEnergyModel, x)
    xsample = Float32.(rand(size(x)...)) .* 2 .- 1
    for i in 1:jem.ld_steps
        xsample = langevine_dynamics_step(jem, xsample)
    end
    return xsample
end
```

## Evaluation and Training

```{julia}
function accuracy(model::JointEnergyModel, x, y; agg=mean)
    ŷ = jem(x)
    mean(onecold(ŷ) .== onecold(y))
end

function evaluation(model::JointEnergyModel, val_set::DataLoader)
    ℓ = 0.0
    ℓ_clf = 0.0
    ℓ_gen = 0.0
    acc = 0.0
    num = 0
    for (x, y) in val_set
        ℓ_clf += sum(class_loss(model, x, y))
        ℓ_gen += sum(gen_loss(model, x))
        ℓ += loss(model, x, y)
        acc += accuracy(model, x, y)
        num += size(x)[end]
    end
    return ℓ / num, ℓ_clf / num, ℓ_gen / num, acc / length(val_set)
end

function samples_real(model::JointEnergyModel, dl::DataLoader, n::Int=16; img_size=n_digits * 10)
    x = reduce((x, y) -> cat(x,y[1],dims=ndims(x)), dl, init = [])
    num_x = Int(round(ceil(sqrt(n))))
    num_y = Int(round(floor(sqrt(n))))
    plot_data = [heatmap(rotl90(x[:,:,rand(1:size(x)[end])]), axis=nothing, cb=false) for i in 1:n]
    plot(plot_data..., layout=(num_x, num_y), size=(num_x*img_size, num_y*img_size), margin=(round(0.05*img_size), :px))
end

function samples_generated(model::JointEnergyModel, dl::DataLoader, n::Int=16; img_size=n_digits * 10)
    x = reduce((x, y) -> cat(x,y[1],dims=ndims(x)), dl, init = [])
    x = sample(model, x)
    num_x = Int(round(ceil(sqrt(n))))
    num_y = Int(round(floor(sqrt(n))))
    plot_data = [heatmap(rotl90(x[:,:,rand(1:size(x)[end])]), axis=nothing, cb=false) for i in 1:n]
    plot(plot_data..., layout=(num_x, num_y), size=(num_x*img_size, num_y*img_size), margin=(round(0.05*img_size), :px))
end
```

```{julia}
function training(
    model::JointEnergyModel, train_set, opt_state; 
    num_epochs::Int=100, val_set::Union{Nothing, DataLoader}=nothing, max_patience::Int=20,
    verbosity::Int=num_epochs
)
    training_log = []
    for epoch in 1:num_epochs
        training_losses = Float32[]

        # Training:
        for (i, data) in enumerate(train_set)

            # Forward pass:
            x, y = data
            val, grads = Flux.withgradient(model) do m
                loss(m, x, y)
            end

            # Save the loss from the forward pass. (Done outside of gradient.)
            push!(training_losses, val)

            # Detect loss of Inf or NaN. Print a warning, and then skip update!
            if !isfinite(val)
                @warn "loss is $val on item $i" epoch
                continue
            end

            Flux.update!(opt_state, model, grads[1])
        end

        # Evluation:
        if !isnothing(val_set)
            ℓ, ℓ_clf, ℓ_gen, acc = evaluation(model, val_set)
            push!(training_log, (; ℓ, ℓ_clf, ℓ_gen, acc, training_losses))
        else
            ℓ, _, _, acc = evaluation(model, train_set)
            push!(training_log, (; ℓ, acc, training_losses))
        end

        # Verbosity:
        if (verbosity > 0) && (epoch % round(num_epochs / verbosity) == 0)
            if isnothing(val_set)
                @info "Training loss in epoch $epoch: $ℓ"
                @info "Training accuracy in epoch $epoch: $acc"
            else
                @info "Validation losses/accuracy in epoch $epoch:"
                println("Classification: $ℓ_clf")
                println("Generative: $ℓ_gen")
                println("Total: $ℓ")
                println("Accuracy: $acc")
            end
        end

        # Early Stopping:
        _loss() = ℓ
        es = Flux.early_stopping(_loss, max_patience)
        es() && break

    end

    return training_log
end
```

## Experiments

### Hyperparameters

```{julia}
D = n_digits                # input dimension
K = 10                      # output dimension
M = 512                   
lr = 1e-3                   # learning rate
num_epochs = 10            
max_patience = 20           # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped
batchsize = 100
```

### Initializing the model

```{julia}
mlp = Chain(
    Flux.flatten,
    Dense(prod((D,D)), M, elu),
    Dense(M, M, elu),
    Dense(M, M, elu),
    Dense(M, K)
)

# We initialize the full model
jem = JointEnergyModel(mlp)
```

### Training loop

```{julia}
# Initialise 
opt = Adam(lr)
opt_state = Flux.setup(opt, jem)
train_set = DataLoader((Xtrain, ytrain); batchsize=batchsize, shuffle=true)
val_set = DataLoader((Xval, yval); batchsize=batchsize, shuffle=false)
test_set = DataLoader((Xtest, ytest); batchsize=batchsize, shuffle=false)
```

```{julia}
training(jem, train_set, opt_state; num_epochs=num_epochs, val_set=val_set)
```

### The final evaluation

```{julia}
ℓ_test, ℓ_clf_test, ℓ_gen_test, acc_test = evaluation(jem, test_set)
```

